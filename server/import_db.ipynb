{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "# Get url from https://www.worldcubeassociation.org/export/results\n",
    "url = input()\n",
    "\n",
    "# If url contains .sql, replace with .tsv\n",
    "url = url.replace('.sql', '.tsv')\n",
    "\n",
    "print(\"Download\")\n",
    "urllib.request.urlretrieve(url, \"WCA_export.zip\")\n",
    "\n",
    "if os.path.exists('WCA_export'):\n",
    "    shutil.rmtree('WCA_export')\n",
    "\n",
    "print(\"Unzip\")\n",
    "shutil.unpack_archive('WCA_export.zip', 'WCA_export')\n",
    "\n",
    "print(\"Rename\")\n",
    "for filename in os.listdir('WCA_export'):\n",
    "    # Remove WCA_export_ from the filename\n",
    "    new_name = filename.replace('WCA_export_', '')\n",
    "    os.rename(f'WCA_export/{filename}', f'WCA_export/{new_name}')\n",
    "\n",
    "print(\"Remove unnecessary\")\n",
    "os.remove('./WCA_export/championships.tsv')\n",
    "os.remove('./WCA_export/eligible_country_iso2s_for_championship.tsv')\n",
    "os.remove('./WCA_export/Formats.tsv')\n",
    "os.remove('./WCA_export/RoundTypes.tsv')\n",
    "os.remove('./WCA_export/Scrambles.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "export_dir = 'WCA_export'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "filenames = os.listdir(export_dir)\n",
    "filenames = [f for f in filenames if f.endswith('.tsv')]\n",
    "tablenames = [filename.split('.')[0] for filename in filenames]\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    tablename = filename.split('.')[0]\n",
    "    dfs[tablename] = pd.read_csv(f'{export_dir}/{filename}', delimiter='\\t')\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Remove unnecessary columns')\n",
    "dfs['Persons'] = dfs['Persons'].drop(columns=['subid'])\n",
    "dfs['Results'] = dfs['Results'].drop(columns=['personName', 'formatId', 'value1', 'value2', 'value3', 'value4', 'value5', 'personCountryId'])\n",
    "dfs['Results'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cast event ids to string')\n",
    "dfs['Events'].id = dfs['Events'].id.astype(str)\n",
    "dfs['RanksSingle'].eventId = dfs['RanksSingle'].eventId.astype(str)\n",
    "dfs['RanksAverage'].eventId = dfs['RanksAverage'].eventId.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Handle duplicate persons')\n",
    "\n",
    "# If a person has moved countries, then they could have multiple entries\n",
    "dfs['Persons'].drop_duplicates('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Populate country ids')\n",
    "country_ids = dfs['Persons'][['id', 'countryId']]\n",
    "\n",
    "dfs['RanksSingle'] = dfs['RanksSingle'].merge(country_ids, left_on='personId', right_on='id').drop('id', axis=1)\n",
    "dfs['RanksAverage'] = dfs['RanksAverage'].merge(country_ids, left_on='personId', right_on='id').drop('id', axis=1)\n",
    "dfs['RanksSingle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Populate continent ids')\n",
    "continent_ids = dfs['Countries'][['id', 'continentId']]\n",
    "\n",
    "dfs['RanksSingle'] = dfs['RanksSingle'].merge(continent_ids, left_on='countryId', right_on='id').drop('id', axis=1)\n",
    "dfs['RanksAverage'] = dfs['RanksAverage'].merge(continent_ids, left_on='countryId', right_on='id').drop('id', axis=1)\n",
    "dfs['Persons'] = dfs['Persons'].merge(continent_ids, left_on='countryId', right_on='id', suffixes=('', '_drop')).drop('id_drop', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Populate names')\n",
    "names = dfs['Persons'][['id', 'name']]\n",
    "\n",
    "dfs['RanksSingle'] = dfs['RanksSingle'].merge(names, left_on='personId', right_on='id').drop('id', axis=1)\n",
    "dfs['RanksAverage'] = dfs['RanksAverage'].merge(names, left_on='personId', right_on='id').drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculate max ranks')\n",
    "single_world_maxes = dfs['RanksSingle'].groupby('eventId').max()['worldRank']\n",
    "average_world_maxes = dfs['RanksAverage'].groupby('eventId').max()['worldRank']\n",
    "\n",
    "single_continent_maxes = dfs['RanksSingle'].groupby(['continentId', 'eventId']).max()['continentRank']\n",
    "average_continent_maxes = dfs['RanksAverage'].groupby(['continentId', 'eventId']).max()['continentRank']\n",
    "\n",
    "single_country_maxes = dfs['RanksSingle'].groupby(['countryId', 'eventId']).max()['countryRank']\n",
    "average_country_maxes = dfs['RanksAverage'].groupby(['countryId', 'eventId']).max()['countryRank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def calculate_sum_of_ranks(type, events, world_maxes, continent_maxes, country_maxes):\n",
    "\n",
    "    person_ids = dfs['Persons'].id.unique()\n",
    "    combinations = pd.DataFrame(list(itertools.product(person_ids, events)), columns=['personId', 'eventId'])\n",
    "\n",
    "    # Make a row for every person and every event. If person has no result, their rank is NaN\n",
    "    ranks_all = combinations \\\n",
    "        .merge(dfs[f'Ranks{type}'][['personId', 'eventId', 'worldRank', 'continentRank', 'countryRank']], on=['personId', 'eventId'], how='left') \\\n",
    "        .merge(dfs['Persons'][['id', 'countryId', 'continentId']], left_on='personId', right_on='id', how='left')\n",
    "\n",
    "    ranks_all['worldRank'] = ranks_all['worldRank'].fillna(ranks_all['eventId'].map(world_maxes))\n",
    "    ranks_all['continentRank'] = ranks_all['continentRank'].fillna(ranks_all.set_index(['continentId', 'eventId']).index.map(continent_maxes).to_series(index=ranks_all.index))\n",
    "    ranks_all['countryRank'] = ranks_all['countryRank'].fillna(ranks_all.set_index(['countryId', 'eventId']).index.map(country_maxes).to_series(index=ranks_all.index))\n",
    "\n",
    "    sor = ranks_all.groupby('personId')[['worldRank', 'continentRank', 'countryRank']].sum()\n",
    "    sor = sor.reset_index()\n",
    "    sor = sor.rename(columns={\n",
    "        'worldRank': f'worldSor{type}',\n",
    "        'continentRank': f'continentSor{type}',\n",
    "        'countryRank': f'countrySor{type}',\n",
    "    })\n",
    "    return sor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_events = ['222', '333', '333bf', '333fm', '333mbf', '333oh', '444', '444bf', '555', '555bf', '666', '777', 'clock', 'minx', 'pyram', 'skewb', 'sq1']\n",
    "\n",
    "# Same as single_events but no 333mbf\n",
    "average_events = ['222', '333', '333bf', '333fm', '333oh', '444', '444bf', '555', '555bf', '666', '777', 'clock', 'minx', 'pyram', 'skewb', 'sq1']\n",
    "\n",
    "print('Single sum of rannks')\n",
    "single_sor = calculate_sum_of_ranks('Single', single_events, single_world_maxes, single_continent_maxes, single_country_maxes)\n",
    "print('Average sum of ranks')\n",
    "average_sor = calculate_sum_of_ranks('Average', average_events, average_world_maxes, average_continent_maxes, average_country_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['Persons'] = dfs['Persons'].merge(single_sor, left_on='id', right_on='personId').drop('personId', axis=1)\n",
    "dfs['Persons'] = dfs['Persons'].merge(average_sor, left_on='id', right_on='personId').drop('personId', axis=1)\n",
    "dfs['Persons'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_singles = dfs['RanksSingle'][dfs['RanksSingle'].worldRank == 1][['eventId', 'best']].rename(columns={'best': 'single'})\n",
    "best_averages = dfs['RanksAverage'][dfs['RanksAverage'].worldRank == 1][['eventId', 'best']].rename(columns={'best': 'average'})\n",
    "\n",
    "world_bests = best_singles.merge(best_averages, on='eventId', how='outer')\n",
    "world_bests = {eventId: (single, average) for eventId, single, average in world_bests.values}\n",
    "world_bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_dict(d):\n",
    "    '''\n",
    "    Example input: {\n",
    "        ('a', 'b'): 1,\n",
    "        ('a', 'c'): 2,\n",
    "    }\n",
    "\n",
    "    Example output: {\n",
    "        'a': {\n",
    "            'b': 1,\n",
    "            'c': 2,\n",
    "        },\n",
    "    }\n",
    "    '''\n",
    "    output = {}\n",
    "    for k in d:\n",
    "        if k[0] not in output:\n",
    "            output[k[0]] = {}\n",
    "        \n",
    "        output[k[0]][k[1]] = d[k]\n",
    "    return output\n",
    "\n",
    "best_singles = dfs['RanksSingle'][dfs['RanksSingle'].continentRank == 1][['eventId', 'best', 'continentId']].rename(columns={'best': 'single'})\n",
    "best_averages = dfs['RanksAverage'][dfs['RanksAverage'].continentRank == 1][['eventId', 'best', 'continentId']].rename(columns={'best': 'average'})\n",
    "\n",
    "continent_bests = best_singles.merge(best_averages, on=['eventId', 'continentId'], how='outer').drop_duplicates()\n",
    "continent_bests = {(continentId, eventId): (single, average) for eventId, single, continentId, average in continent_bests.values}\n",
    "continent_bests = group_dict(continent_bests)\n",
    "\n",
    "best_singles = dfs['RanksSingle'][dfs['RanksSingle'].countryRank == 1][['eventId', 'best', 'countryId']].rename(columns={'best': 'single'})\n",
    "best_averages = dfs['RanksAverage'][dfs['RanksAverage'].countryRank == 1][['eventId', 'best', 'countryId']].rename(columns={'best': 'average'})\n",
    "\n",
    "country_bests = best_singles.merge(best_averages, on=['eventId', 'countryId'], how='outer').drop_duplicates()\n",
    "country_bests = {(continentId, eventId): (single, average) for eventId, single, continentId, average in country_bests.values}\n",
    "country_bests = group_dict(country_bests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rank_dict(tablename):\n",
    "    d = {}\n",
    "    for row in dfs[tablename][['personId', 'eventId', 'best']].values:\n",
    "        personId, eventId, best = row\n",
    "\n",
    "        if personId not in d:\n",
    "            d[personId] = {}\n",
    "\n",
    "        d[personId][eventId] = best\n",
    "    \n",
    "    # Make sure every person at least has an empty object\n",
    "    for id in dfs['Persons']['id']:\n",
    "        if id not in d:\n",
    "            d[id] = {}\n",
    "\n",
    "    return d\n",
    "\n",
    "print('Build rank dicts')\n",
    "single_dict = build_rank_dict('RanksSingle')\n",
    "average_dict = build_rank_dict('RanksAverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def mbldScore(value):\n",
    "    if not value:\n",
    "        return 0\n",
    "    seconds = math.floor(value / 100) % 1e5\n",
    "    points = 99 - (math.floor(value / 1e7) % 100)\n",
    "    centiseconds = None if seconds == 99999 else seconds * 100\n",
    "    proportionOfHourLeft = 1 - centiseconds / 360000\n",
    "    score = points + proportionOfHourLeft\n",
    "    return max(score, 0)\n",
    "\n",
    "def get_kinch_score(personId, bests, key):\n",
    "    if key:\n",
    "        if key in bests:\n",
    "            bests = bests[key]\n",
    "        else:\n",
    "            # This edge case can occur if, for example, a person moves to a new country that has no results.\n",
    "            # This occurred for wca id 2018YEDD01 who moved to Barbados!\n",
    "            return 0\n",
    "    scores = []\n",
    "\n",
    "    # Handle 333mbf\n",
    "    single = single_dict[personId].get(\"333mbf\")\n",
    "    average = average_dict[personId].get(\"333mbf\")\n",
    "    bestSingle, bestAverage = bests[\"333mbf\"] if \"333mbf\" in bests else (None, None)\n",
    "\n",
    "    mbldPersonal = mbldScore(single)\n",
    "    mbldRecord = mbldScore(bestSingle)\n",
    "\n",
    "    if mbldRecord:\n",
    "        scores.append(mbldPersonal / mbldRecord * 100)\n",
    "    else:\n",
    "        # If nobody has mbld, use 100\n",
    "        scores.append(0)\n",
    "\n",
    "    # For these events, use better between single and average\n",
    "    for eventId in [\"333fm\", \"333bf\", \"444bf\", \"555bf\"]:\n",
    "        single = single_dict[personId].get(eventId)\n",
    "        average = average_dict[personId].get(eventId)\n",
    "        bestSingle, bestAverage = bests[eventId] if eventId in bests else (None, None)\n",
    "\n",
    "        if not single and not average:\n",
    "            scores.append(0)\n",
    "        elif not bestSingle or not bestAverage:\n",
    "            # This can happen if a person has multiple countryIds and one of the countries has no result for the event.\n",
    "            scores.append(100)\n",
    "        elif not average:\n",
    "            # If no average, use single\n",
    "            scores.append(bestSingle / single * 100)\n",
    "        else:\n",
    "            # If there is an average, use the better of the two\n",
    "            scores.append(max(\n",
    "                bestSingle / single * 100,\n",
    "                bestAverage / average * 100\n",
    "            ))\n",
    "\n",
    "    # For these events, use average\n",
    "    for eventId in ['222', '333', '333oh', '444', '555', '666', '777', 'clock', 'minx', 'pyram', 'skewb', 'sq1']:\n",
    "        single = single_dict[personId].get(eventId)\n",
    "        average = average_dict[personId].get(eventId)\n",
    "        bestSingle, bestAverage = bests[eventId] if eventId in bests else (None, None)\n",
    "\n",
    "        if not average:\n",
    "            scores.append(0)\n",
    "        elif not bestAverage:\n",
    "            # This can happen if a person has multiple countryIds and one of the countries has no result for the event.\n",
    "            scores.append(100)\n",
    "        else:\n",
    "            scores.append(bestAverage / average * 100)\n",
    "\n",
    "    avgScore = sum(scores) / len(scores)\n",
    "    return avgScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = dfs['Persons']\n",
    "\n",
    "print('World kinch')\n",
    "persons['worldKinch'] = persons.apply(lambda row: get_kinch_score(row['id'], world_bests, None), axis=1)\n",
    "\n",
    "print('Continent kinch')\n",
    "persons['continentKinch'] = persons.apply(lambda row: get_kinch_score(row['id'], continent_bests, row['continentId']), axis=1)\n",
    "\n",
    "print('Country kinch')\n",
    "persons['countryKinch'] = persons.apply(lambda row: get_kinch_score(row['id'], country_bests, row['countryId']), axis=1)\n",
    "\n",
    "dfs['Persons'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Populate startDate and endDate')\n",
    "\n",
    "# Format is yyyy-mm-dd\n",
    "# Pad month and day with zeros\n",
    "dfs['Competitions']['startDate'] = dfs['Competitions'].apply(lambda row: f\"{row['year']}-{str(row['month']).zfill(2)}-{str(row['day']).zfill(2)}\", axis=1)\n",
    "dfs['Competitions']['endDate'] = dfs['Competitions'].apply(lambda row: f\"{row['year']}-{str(row['endMonth']).zfill(2)}-{str(row['endDay']).zfill(2)}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculate birthdays')\n",
    "\n",
    "# Get competition data\n",
    "comps = dfs['Results'][['competitionId', 'personId']].drop_duplicates()\n",
    "comps = comps.merge(dfs['Competitions'][['id', 'startDate']], left_on='competitionId', right_on='id').drop('id', axis=1)\n",
    "comps = comps.merge(dfs['Persons'][['id', 'name']], left_on='personId', right_on='id').drop('id', axis=1)\n",
    "comps = comps.sort_values('startDate')\n",
    "\n",
    "# Get first comp for each person\n",
    "first_comps = {}\n",
    "for row in comps.values:\n",
    "    personId = row[1]\n",
    "\n",
    "    if personId in first_comps:\n",
    "        continue\n",
    "\n",
    "    first_comps[personId] = row\n",
    "\n",
    "def sort_dict(d, keys):\n",
    "    output = {}\n",
    "    for key in keys:\n",
    "        output[key] = d[key]\n",
    "    return output\n",
    "\n",
    "# Get persons in order of rank\n",
    "persons = dfs['RanksSingle'].sort_values('worldRank')['personId'].unique()\n",
    "\n",
    "first_comps = sort_dict(first_comps, persons)\n",
    "\n",
    "dfs['Birthdays'] = pd.DataFrame(first_comps.values(), columns=['competitionId', 'personId', 'date', 'name'])\n",
    "dfs['Birthdays'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'{export_dir}/metadata.json', 'r') as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "    dfs['Miscellaneous'] = pd.DataFrame({\n",
    "        'key': ['export_date'],\n",
    "        'value': [data['export_date'][0:10]], # Only first 10 chars for yyyy-mm-dd\n",
    "    })\n",
    "\n",
    "dfs['Miscellaneous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def df_to_sqlite(df, table_name):\n",
    "    conn = sqlite3.connect('wca.db')\n",
    "\n",
    "    try:\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        print(f\"{table_name} table created\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Remove wca.db\n",
    "if os.path.exists('wca.db'):\n",
    "    os.remove('wca.db')\n",
    "\n",
    "for name in dfs:\n",
    "    df_to_sqlite(dfs[name], name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Create indices')\n",
    "conn = sqlite3.connect('wca.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# TODO: Analyze which of these indices are actually needed\n",
    "c.execute('CREATE INDEX idx_Persons_id ON Persons(id);')\n",
    "c.execute('CREATE INDEX idx_Persons_countryId ON Persons(countryId);')\n",
    "c.execute('CREATE INDEX idx_Persons_continentId ON Persons(continentId);')\n",
    "c.execute('CREATE INDEX idx_Persons_countryKinch ON Persons(countryKinch);')\n",
    "c.execute('CREATE INDEX idx_Persons_continentKinch ON Persons(continentKinch);')\n",
    "c.execute('CREATE INDEX idx_Persons_worldKinch ON Persons(worldKinch);')\n",
    "c.execute('CREATE INDEX idx_Persons_countrySorSingle ON Persons(countrySorSingle);')\n",
    "c.execute('CREATE INDEX idx_Persons_continentSorSingle ON Persons(continentSorSingle);')\n",
    "c.execute('CREATE INDEX idx_Persons_worldSorSingle ON Persons(worldSorSingle);')\n",
    "c.execute('CREATE INDEX idx_Persons_countrySorAverage ON Persons(countrySorAverage);')\n",
    "c.execute('CREATE INDEX idx_Persons_continentSorAverage ON Persons(continentSorAverage);')\n",
    "c.execute('CREATE INDEX idx_Persons_worldSorAverage ON Persons(worldSorAverage);')\n",
    "c.execute('CREATE INDEX idx_RanksSingle_eventId ON RanksSingle(eventId);')\n",
    "c.execute('CREATE INDEX idx_RanksSingle_personId ON RanksSingle(personId);')\n",
    "c.execute('CREATE INDEX idx_RanksSingle_worldRank ON RanksSingle(worldRank);')\n",
    "c.execute('CREATE INDEX idx_RanksSingle_continentRank ON RanksSingle(continentRank);')\n",
    "c.execute('CREATE INDEX idx_RanksSingle_countryRank ON RanksSingle(countryRank);')\n",
    "c.execute('CREATE INDEX idx_RanksAverage_eventId ON RanksAverage(eventId);')\n",
    "c.execute('CREATE INDEX idx_RanksAverage_personId ON RanksAverage(personId);')\n",
    "c.execute('CREATE INDEX idx_RanksAverage_worldRank ON RanksAverage(worldRank);')\n",
    "c.execute('CREATE INDEX idx_RanksAverage_continentRank ON RanksAverage(continentRank);')\n",
    "c.execute('CREATE INDEX idx_RanksAverage_countryRank ON RanksAverage(countryRank);')\n",
    "c.execute('CREATE INDEX idx_Staff_wca_id ON Staff(wca_id);')\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
